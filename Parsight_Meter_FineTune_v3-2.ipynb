{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61ccee01e9204c379adf89924b06b339": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_317a6fd722064450ab1a52ebc4bd425c",
              "IPY_MODEL_534178a994f14329b3fce1ed5eb8baef",
              "IPY_MODEL_0dc752cd5b06442ca425206ae75f84fe"
            ],
            "layout": "IPY_MODEL_339653e486874fec95f10293452bd521"
          }
        },
        "317a6fd722064450ab1a52ebc4bd425c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_979997b2084340fd82704cf714742f88",
            "placeholder": "​",
            "style": "IPY_MODEL_bfe4e7d7196f42d8b07796b2e8104e07",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "534178a994f14329b3fce1ed5eb8baef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c956cc84432e49c98da57afe6f72969b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f8c7a9019ce455c830ffd28e67067a8",
            "value": 2
          }
        },
        "0dc752cd5b06442ca425206ae75f84fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8666b831a91345948a199206c18051c2",
            "placeholder": "​",
            "style": "IPY_MODEL_1c384c7355bc4de28c105f828ec521ad",
            "value": " 2/2 [00:14&lt;00:00,  6.37s/it]"
          }
        },
        "339653e486874fec95f10293452bd521": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979997b2084340fd82704cf714742f88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfe4e7d7196f42d8b07796b2e8104e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c956cc84432e49c98da57afe6f72969b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8c7a9019ce455c830ffd28e67067a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8666b831a91345948a199206c18051c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c384c7355bc4de28c105f828ec521ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlgoMJYaZqt"
      },
      "source": [
        "# Parsight Meter Reading — Fine-Tune Qwen2-VL-2B\n",
        "\n",
        "**This notebook fine-tunes Qwen2-VL-2B-Instruct on your 75 gas meter images.**\n",
        "\n",
        "### Key design decisions:\n",
        "- **NO bitsandbytes** — avoids the `triton.ops` error on Colab T4\n",
        "- **float16 instead of 4-bit** — Qwen2-VL-2B is only ~4GB, fits on T4 (16GB) in float16\n",
        "- **LoRA** — only trains 1% of parameters (~20M out of 2B)\n",
        "- **Gradient checkpointing** — saves ~40% VRAM by recomputing activations\n",
        "\n",
        "### Cost after training:\n",
        "| Method | Cost/image |\n",
        "|--------|----------|\n",
        "| GPT-4o (current) | $0.10 |\n",
        "| This fine-tuned model | ~$0.001 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q4FNY0qaZqw"
      },
      "source": [
        "---\n",
        "## Cell 1: Check GPU\n",
        "\n",
        "**WHY:** Confirms a GPU is available and checks VRAM to set batch size.\n",
        "- T4 (16GB free tier): batch_size=1\n",
        "- A100 (40GB Pro): batch_size=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY7YxpmwaZqw",
        "outputId": "45b8087f-ff2e-4893-e102-80658f75f030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "VRAM: 15.6 GB\n",
            "Batch: 1, Grad accum: 8, Effective batch: 8\n",
            "✅ GPU ready!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\n",
        "        \"No GPU! Go to Runtime → Change runtime type → GPU (T4 or A100)\"\n",
        "    )\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "\n",
        "print(f\"GPU: {gpu_name}\")\n",
        "print(f\"VRAM: {gpu_mem:.1f} GB\")\n",
        "\n",
        "if gpu_mem >= 35:\n",
        "    BATCH_SIZE = 2\n",
        "    GRAD_ACCUM = 4\n",
        "elif gpu_mem >= 20:\n",
        "    BATCH_SIZE = 1\n",
        "    GRAD_ACCUM = 8\n",
        "else:\n",
        "    BATCH_SIZE = 1\n",
        "    GRAD_ACCUM = 8\n",
        "\n",
        "print(f\"Batch: {BATCH_SIZE}, Grad accum: {GRAD_ACCUM}, Effective batch: {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(\"✅ GPU ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be1AdoUXaZqx"
      },
      "source": [
        "---\n",
        "## Cell 2: Install Dependencies\n",
        "\n",
        "**WHY each package:**\n",
        "- `transformers` — Has the Qwen2-VL model code\n",
        "- `peft` — LoRA fine-tuning (train 1% of weights)\n",
        "- `accelerate` — GPU memory management\n",
        "- `qwen-vl-utils` — Qwen's image processing\n",
        "- `datasets` — Data loading\n",
        "\n",
        "**NOTE: We do NOT install bitsandbytes.** The 2B model fits in float16 on T4 (~4GB model + ~10GB training = 14GB < 16GB). This avoids the `triton.ops` error entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsGFHGaHaZqx",
        "outputId": "6cc77b8a-8cec-45a7-eb97-15f0fd154cac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "✅ All packages installed (no bitsandbytes needed!)\n"
          ]
        }
      ],
      "source": [
        "# NO bitsandbytes - avoids triton.ops error on Colab T4\n",
        "!pip install -q \\\n",
        "    transformers==4.46.3 \\\n",
        "    peft==0.13.2 \\\n",
        "    accelerate==1.1.1 \\\n",
        "    \"qwen-vl-utils==0.0.8\" \\\n",
        "    datasets==3.1.0\n",
        "\n",
        "print(\"\\n✅ All packages installed (no bitsandbytes needed!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pzp24UvaZqy"
      },
      "source": [
        "---\n",
        "## Cell 3: Upload & Extract Dataset\n",
        "\n",
        "**WHY:** Uploads your `parsight_complete_dataset_75.zip` to Colab's temporary disk and extracts the images + JSONL files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L4Kp79WCaZqy",
        "outputId": "7e6df3e6-76dc-4121-b5a0-dd0e29432353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload your parsight_complete_dataset_75.zip:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-badf8c40-c967-44c1-95a8-ed6f830a124d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-badf8c40-c967-44c1-95a8-ed6f830a124d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving complete_dataset.zip to complete_dataset.zip\n",
            "\n",
            "Uploaded: complete_dataset.zip (2.2 MB)\n",
            "Dataset dir: /content/dataset/complete_dataset\n",
            "Images: 75\n",
            "Train: 67 | Val: 8\n",
            "✅ Dataset ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload your parsight_complete_dataset_75.zip:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_name = list(uploaded.keys())[0]\n",
        "print(f\"\\nUploaded: {zip_name} ({len(uploaded[zip_name]) / 1e6:.1f} MB)\")\n",
        "\n",
        "DATASET_DIR = \"/content/dataset\"\n",
        "with zipfile.ZipFile(zip_name, 'r') as z:\n",
        "    z.extractall(DATASET_DIR)\n",
        "\n",
        "# Find the actual data directory (may be nested)\n",
        "for root, dirs, fnames in os.walk(DATASET_DIR):\n",
        "    if 'train.jsonl' in fnames:\n",
        "        DATASET_DIR = root\n",
        "        break\n",
        "\n",
        "print(f\"Dataset dir: {DATASET_DIR}\")\n",
        "print(f\"Images: {len(os.listdir(os.path.join(DATASET_DIR, 'images')))}\")\n",
        "\n",
        "with open(os.path.join(DATASET_DIR, 'train.jsonl')) as f:\n",
        "    train_count = sum(1 for _ in f)\n",
        "with open(os.path.join(DATASET_DIR, 'val.jsonl')) as f:\n",
        "    val_count = sum(1 for _ in f)\n",
        "\n",
        "print(f\"Train: {train_count} | Val: {val_count}\")\n",
        "print(\"✅ Dataset ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWKQAEAtaZqy"
      },
      "source": [
        "---\n",
        "## Cell 4: Load Base Model (float16, NO quantization)\n",
        "\n",
        "**WHY float16 instead of 4-bit:**\n",
        "- Qwen2-VL-2B in float16 = ~4GB VRAM\n",
        "- T4 has 16GB VRAM → plenty of room for training overhead\n",
        "- No need for bitsandbytes/triton (which causes errors on Colab)\n",
        "- float16 is actually BETTER quality than 4-bit (no quantization loss)\n",
        "\n",
        "**WHY `device_map=\"auto\"`:** Automatically places model on GPU.\n",
        "\n",
        "**WHY min/max_pixels:** Controls image resolution sent to the model.\n",
        "- Too small → can't read meter digits\n",
        "- Too large → VRAM explodes\n",
        "- 256-512px is the sweet spot for meter photos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "61ccee01e9204c379adf89924b06b339",
            "317a6fd722064450ab1a52ebc4bd425c",
            "534178a994f14329b3fce1ed5eb8baef",
            "0dc752cd5b06442ca425206ae75f84fe",
            "339653e486874fec95f10293452bd521",
            "979997b2084340fd82704cf714742f88",
            "bfe4e7d7196f42d8b07796b2e8104e07",
            "c956cc84432e49c98da57afe6f72969b",
            "4f8c7a9019ce455c830ffd28e67067a8",
            "8666b831a91345948a199206c18051c2",
            "1c384c7355bc4de28c105f828ec521ad"
          ]
        },
        "id": "mU2deffWaZqy",
        "outputId": "4e12e653-5738-4bb4-e180-12cdb1c3a787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Qwen/Qwen2-VL-2B-Instruct in float16 (no quantization needed for 2B model)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61ccee01e9204c379adf89924b06b339"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "GPU memory: 8.8 / 15.6 GB used\n",
            "Free for training: 6.8 GB\n",
            "✅ Base model loaded!\n"
          ]
        }
      ],
      "source": [
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "import torch\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "\n",
        "print(f\"Loading {MODEL_ID} in float16 (no quantization needed for 2B model)...\")\n",
        "\n",
        "# Load model in float16 — NO bitsandbytes, NO 4-bit\n",
        "# WHY torch_dtype=float16: Half precision, 2x smaller than float32\n",
        "# WHY attn_implementation=\"eager\": Avoids flash_attention issues on T4\n",
        "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",  # T4 doesn't support flash_attention_2\n",
        ")\n",
        "\n",
        "# Load processor (handles image resizing + text tokenization)\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    min_pixels=256 * 28 * 28,   # ~200K pixels min\n",
        "    max_pixels=512 * 28 * 28,   # ~400K pixels max\n",
        ")\n",
        "\n",
        "mem_used = torch.cuda.memory_allocated() / 1e9\n",
        "mem_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "print(f\"\\nGPU memory: {mem_used:.1f} / {mem_total:.1f} GB used\")\n",
        "print(f\"Free for training: {mem_total - mem_used:.1f} GB\")\n",
        "print(\"✅ Base model loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP8NbDD9aZqy"
      },
      "source": [
        "---\n",
        "## Cell 5: Configure LoRA\n",
        "\n",
        "**WHY LoRA:**\n",
        "- 2B params total, but we only train ~20M (1%)\n",
        "- Prevents overfitting on just 75 images\n",
        "- Much faster than full fine-tuning\n",
        "\n",
        "**WHY these settings:**\n",
        "- `r=16` — Adapter rank. Higher=more capacity but more overfitting\n",
        "- `lora_alpha=32` — Scaling factor = 2×r (standard)\n",
        "- `lora_dropout=0.1` — 10% dropout, critical with only 75 images\n",
        "- `target_modules` — Attention layers where model decides \"what to look at\"\n",
        "\n",
        "**WHY `gradient_checkpointing`:**\n",
        "- Saves ~40% VRAM by recomputing activations during backward pass\n",
        "- Makes training ~20% slower but prevents OOM on T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3wP_ZklaZqz",
        "outputId": "17e9bae4-91dc-4f32-f16b-30ed616c8112"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params:     2,213,343,744\n",
            "Trainable params:    4,358,144\n",
            "Trainable %:      0.20%\n",
            "\n",
            "✅ LoRA ready! Training 4.4M of 2213M params.\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Enable gradient checkpointing BEFORE applying LoRA\n",
        "# WHY: Saves ~40% VRAM by recomputing activations instead of storing them\n",
        "model.gradient_checkpointing_enable(\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
        ")\n",
        "\n",
        "# Enable input gradients (required for gradient checkpointing)\n",
        "model.enable_input_require_grads()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "trainable, total = model.get_nb_trainable_parameters()\n",
        "print(f\"Total params:     {total:>12,}\")\n",
        "print(f\"Trainable params: {trainable:>12,}\")\n",
        "print(f\"Trainable %:      {trainable/total*100:.2f}%\")\n",
        "print(f\"\\n✅ LoRA ready! Training {trainable/1e6:.1f}M of {total/1e6:.0f}M params.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1jr8-WYaZqz"
      },
      "source": [
        "---\n",
        "## Cell 6: Prepare Training Data\n",
        "\n",
        "**WHY this custom Dataset class:**\n",
        "- Loads each JSONL entry + its image from disk\n",
        "- Formats into Qwen2-VL's chat template (`<|im_start|>user...`)\n",
        "- Tokenizes text + processes image together\n",
        "- Masks the user prompt in labels (set to -100) so the model only learns to predict the assistant's JSON response\n",
        "\n",
        "**WHY mask labels:**\n",
        "The model should learn to OUTPUT the JSON reading, not to parrot the input question. We set user tokens to -100 (PyTorch's \"ignore\" index) so loss is only computed on the assistant's answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsENA16uaZqz",
        "outputId": "c75eed1a-7731-4fa5-b2a9-689f8c0c2245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 67 examples from train.jsonl\n",
            "Loaded 8 examples from val.jsonl\n",
            "\n",
            "Sample check:\n",
            "  input_ids: torch.Size([434])\n",
            "  pixel_values: torch.Size([1064, 1176])\n",
            "  Masked tokens: 329\n",
            "  Target tokens: 105\n",
            "✅ Datasets ready!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MeterReadingDataset(Dataset):\n",
        "    def __init__(self, jsonl_path, dataset_dir, processor):\n",
        "        self.dataset_dir = dataset_dir\n",
        "        self.processor = processor\n",
        "        self.examples = []\n",
        "        with open(jsonl_path, 'r') as f:\n",
        "            for line in f:\n",
        "                self.examples.append(json.loads(line.strip()))\n",
        "        print(f\"Loaded {len(self.examples)} examples from {os.path.basename(jsonl_path)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.examples[idx]\n",
        "\n",
        "        # Load image\n",
        "        image_path = os.path.join(self.dataset_dir, example['image'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        user_text = example['conversations'][0]['content']\n",
        "        assistant_text = example['conversations'][1]['content']\n",
        "\n",
        "        # Format as Qwen2-VL messages\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": user_text},\n",
        "                ],\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": assistant_text},\n",
        "                ],\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Apply Qwen2-VL chat template\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "\n",
        "        # Tokenize text + process image\n",
        "        inputs = self.processor(\n",
        "            text=[text],\n",
        "            images=[image],\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Remove batch dim (DataLoader adds it back)\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "\n",
        "        # Create labels: mask user prompt with -100, keep assistant response\n",
        "        input_ids = inputs['input_ids']\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Find where assistant response starts\n",
        "        assistant_marker = self.processor.tokenizer.encode(\n",
        "            '<|im_start|>assistant\\n', add_special_tokens=False\n",
        "        )\n",
        "        marker_len = len(assistant_marker)\n",
        "\n",
        "        ids_list = input_ids.tolist()\n",
        "        assistant_start = -1\n",
        "        for i in range(len(ids_list) - marker_len + 1):\n",
        "            if ids_list[i:i + marker_len] == assistant_marker:\n",
        "                assistant_start = i + marker_len\n",
        "                break\n",
        "\n",
        "        if assistant_start > 0:\n",
        "            labels[:assistant_start] = -100  # Mask user prompt\n",
        "\n",
        "        inputs['labels'] = labels\n",
        "        return inputs\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = MeterReadingDataset(\n",
        "    os.path.join(DATASET_DIR, 'train.jsonl'), DATASET_DIR, processor\n",
        ")\n",
        "val_dataset = MeterReadingDataset(\n",
        "    os.path.join(DATASET_DIR, 'val.jsonl'), DATASET_DIR, processor\n",
        ")\n",
        "\n",
        "# Verify\n",
        "sample = train_dataset[0]\n",
        "print(f\"\\nSample check:\")\n",
        "print(f\"  input_ids: {sample['input_ids'].shape}\")\n",
        "print(f\"  pixel_values: {sample['pixel_values'].shape}\")\n",
        "print(f\"  Masked tokens: {(sample['labels'] == -100).sum().item()}\")\n",
        "print(f\"  Target tokens: {(sample['labels'] != -100).sum().item()}\")\n",
        "print(\"✅ Datasets ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9VDTiONaZqz"
      },
      "source": [
        "---\n",
        "## Cell 7: Data Collator\n",
        "\n",
        "**WHY custom collator:**\n",
        "Different images produce different token sequence lengths. The collator pads shorter sequences so they can be batched together. Standard collators don't handle Qwen2-VL's `image_grid_thw` tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0MELA-vaZqz",
        "outputId": "131ff9fc-5567-48bc-ddf4-1a8bd1c5b122"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Collator ready!\n"
          ]
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class MeterCollator:\n",
        "    processor: object\n",
        "\n",
        "    def __call__(self, features: List[Dict]) -> Dict:\n",
        "        max_len = max(f['input_ids'].shape[0] for f in features)\n",
        "        pad_token_id = self.processor.tokenizer.pad_token_id or 0\n",
        "\n",
        "        batch = {\n",
        "            'input_ids': [], 'attention_mask': [], 'labels': [],\n",
        "            'pixel_values': [], 'image_grid_thw': [],\n",
        "        }\n",
        "\n",
        "        for f in features:\n",
        "            pad_len = max_len - f['input_ids'].shape[0]\n",
        "            if pad_len > 0:\n",
        "                batch['input_ids'].append(\n",
        "                    torch.cat([f['input_ids'], torch.full((pad_len,), pad_token_id)]))\n",
        "                batch['attention_mask'].append(\n",
        "                    torch.cat([f['attention_mask'], torch.zeros(pad_len, dtype=torch.long)]))\n",
        "                batch['labels'].append(\n",
        "                    torch.cat([f['labels'], torch.full((pad_len,), -100)]))\n",
        "            else:\n",
        "                batch['input_ids'].append(f['input_ids'])\n",
        "                batch['attention_mask'].append(f['attention_mask'])\n",
        "                batch['labels'].append(f['labels'])\n",
        "\n",
        "            batch['pixel_values'].append(f['pixel_values'])\n",
        "            batch['image_grid_thw'].append(f['image_grid_thw'])\n",
        "\n",
        "        batch['input_ids'] = torch.stack(batch['input_ids'])\n",
        "        batch['attention_mask'] = torch.stack(batch['attention_mask'])\n",
        "        batch['labels'] = torch.stack(batch['labels'])\n",
        "        batch['pixel_values'] = torch.cat(batch['pixel_values'], dim=0)\n",
        "        batch['image_grid_thw'] = torch.stack(batch['image_grid_thw'], dim=0) # Changed from cat to stack\n",
        "\n",
        "        return batch\n",
        "\n",
        "\n",
        "collator = MeterCollator(processor=processor)\n",
        "print(\"✅ Collator ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIq-wyAYaZq0"
      },
      "source": [
        "---\n",
        "## Cell 8: Training Config\n",
        "\n",
        "**WHY these hyperparameters:**\n",
        "- `epochs=3` — 67 images × 3 = ~201 update steps. 1=underfitting, 5+=overfitting\n",
        "- `lr=2e-4` — Standard for LoRA. From QLoRA paper.\n",
        "- `warmup_ratio=0.1` — First 10% uses ramping LR to prevent early instability\n",
        "- `cosine scheduler` — LR decays smoothly to 0\n",
        "- `fp16=True` — 2× faster training\n",
        "- `gradient_checkpointing=True` — Saves ~40% VRAM\n",
        "- `optim=\"adamw_torch\"` — Standard optimizer (NOT 8-bit since no bitsandbytes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7wULNAgaZq0",
        "outputId": "956111ae-056f-412a-c7e9-2e65b40c86f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 3\n",
            "Effective batch: 1 × 8 = 8\n",
            "LR: 0.0002\n",
            "Total steps: ~25\n",
            "✅ Training config ready!\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "OUTPUT_DIR = \"/content/parsight_meter_model\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "\n",
        "    # Training duration\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    # Batch size\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "\n",
        "    # Learning rate\n",
        "    learning_rate=2e-4,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    # Memory optimization\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"adamw_torch\",       # Standard optimizer (no bitsandbytes needed)\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    # Logging & saving\n",
        "    logging_steps=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "\n",
        "    # Other\n",
        "    remove_unused_columns=False,   # Keep pixel_values, image_grid_thw\n",
        "    dataloader_pin_memory=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Effective batch: {BATCH_SIZE} × {GRAD_ACCUM} = {BATCH_SIZE * GRAD_ACCUM}\")\n",
        "print(f\"LR: {training_args.learning_rate}\")\n",
        "print(f\"Total steps: ~{len(train_dataset) * 3 // (BATCH_SIZE * GRAD_ACCUM)}\")\n",
        "print(\"✅ Training config ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DQ7v9VNaZq0"
      },
      "source": [
        "---\n",
        "## Cell 9: Train!\n",
        "\n",
        "**WHY `Trainer`:** Handles gradient accumulation, mixed precision, checkpointing, evaluation, and saving — would be ~200 lines manually.\n",
        "\n",
        "**Expected time:** ~30-60 min on T4, ~15 min on A100.\n",
        "\n",
        "**Watch the loss:** Should decrease. If it increases after epoch 2, model is overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "R6R84WxWaZq0",
        "outputId": "0ec7b187-d8b5-4271-ea36-9e5554739d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Watch the loss — should decrease over time.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 02:27, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.070400</td>\n",
              "      <td>1.772646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.165500</td>\n",
              "      <td>0.846862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.722700</td>\n",
              "      <td>0.655273</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TRAINING COMPLETE!\n",
            "  Final train loss: 1.5761\n",
            "  Training time: 154 seconds\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Validation loss: 0.6553\n",
            "\n",
            "✅ Training done!\n"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(\"Watch the loss — should decrease over time.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "train_result = trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(f\"  Final train loss: {train_result.training_loss:.4f}\")\n",
        "print(f\"  Training time: {train_result.metrics['train_runtime']:.0f} seconds\")\n",
        "\n",
        "eval_result = trainer.evaluate()\n",
        "print(f\"  Validation loss: {eval_result['eval_loss']:.4f}\")\n",
        "print(\"\\n✅ Training done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNsTVhPvaZq0"
      },
      "source": [
        "---\n",
        "## Cell 10: Save Model\n",
        "\n",
        "**WHY save adapters only:** Full model = ~4GB. LoRA adapters = ~80MB. We save just the adapters and load them on top of the base model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZA-60rkaZq1",
        "outputId": "cdcc45cc-e2a0-4961-a30e-e00752634998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to: /content/parsight_meter_lora\n",
            "  README.md                                     0.0 MB\n",
            "  adapter_config.json                           0.0 MB\n",
            "  adapter_model.safetensors                     17.5 MB\n",
            "  added_tokens.json                             0.0 MB\n",
            "  chat_template.json                            0.0 MB\n",
            "  merges.txt                                    1.7 MB\n",
            "  preprocessor_config.json                      0.0 MB\n",
            "  special_tokens_map.json                       0.0 MB\n",
            "  tokenizer.json                                11.4 MB\n",
            "  tokenizer_config.json                         0.0 MB\n",
            "  vocab.json                                    2.8 MB\n",
            "\n",
            "  Total: 33.3 MB\n",
            "✅ Model saved!\n"
          ]
        }
      ],
      "source": [
        "SAVE_DIR = \"/content/parsight_meter_lora\"\n",
        "\n",
        "model.save_pretrained(SAVE_DIR)\n",
        "processor.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"Saved to: {SAVE_DIR}\")\n",
        "total_size = 0\n",
        "for f in sorted(os.listdir(SAVE_DIR)):\n",
        "    fpath = os.path.join(SAVE_DIR, f)\n",
        "    if os.path.isfile(fpath):\n",
        "        size = os.path.getsize(fpath)\n",
        "        total_size += size\n",
        "        print(f\"  {f:45s} {size/1e6:.1f} MB\")\n",
        "print(f\"\\n  Total: {total_size/1e6:.1f} MB\")\n",
        "print(\"✅ Model saved!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvDs-hPRaZq1"
      },
      "source": [
        "---\n",
        "## Cell 11: Test on Validation Image\n",
        "\n",
        "**WHY test before downloading:** Verify the model actually learned to read meters before spending time downloading and deploying."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this in a new Colab cell\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip the LoRA folder\n",
        "shutil.make_archive(\"/content/parsight_meter_lora\", 'zip', \"/content/parsight_meter_lora\")\n",
        "\n",
        "# Download\n",
        "files.download(\"/content/parsight_meter_lora.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "yWmndzxFdQ_e",
        "outputId": "e24d6532-ff18-47d0-dcbb-7016f799fca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_92cec076-d08f-4f51-b84b-fea9ac668644\", \"parsight_meter_lora.zip\", 19972239)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktxk1jOTaZq1",
        "outputId": "4205163e-d334-41dd-84be-a638032eb768"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing: images/meter_012.png\n",
            "Expected: {\"meter_number\": \"SMTT7731934LES5\", \"bp_number\": \"789243576\", \"full_reading\": 347.215, \"billing_read...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model output:\n",
            "```json\n",
            "{\n",
            "  \"meter_reading\": \"000347.215\",\n",
            "  \"bp_number\": \"BP-789243576\",\n",
            "  \"full_reading\": \"000347.215 m³\",\n",
            "  \"billing_reading\": \"215\",\n",
            "  \"meter_type\": \"ALPS G1.6\",\n",
            "  \"manufacturer\": \"SMART\",\n",
            "  \"black_digit\": \"000\",\n",
            "  \"red_digit\": \"347\",\n",
            "  \"customer_name\": \"KHAN-503 OKHLA\"\n",
            "}\n",
            "```\n",
            "\n",
            "⚠️ Output is not valid JSON. May need more training.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "\n",
        "# Load first validation example\n",
        "with open(os.path.join(DATASET_DIR, 'val.jsonl')) as f:\n",
        "    test_example = json.loads(f.readline())\n",
        "\n",
        "test_image_path = os.path.join(DATASET_DIR, test_example['image'])\n",
        "test_image = Image.open(test_image_path).convert('RGB')\n",
        "expected = test_example['conversations'][1]['content']\n",
        "\n",
        "print(f\"Testing: {test_example['image']}\")\n",
        "print(f\"Expected: {expected[:100]}...\")\n",
        "\n",
        "# Create inference prompt\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": test_image},\n",
        "            {\"type\": \"text\", \"text\": test_example['conversations'][0]['content']},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "text = processor.apply_chat_template(\n",
        "    messages, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = processor(\n",
        "    text=[text], images=[test_image], return_tensors=\"pt\", padding=True\n",
        ").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(\n",
        "        **inputs, max_new_tokens=256, do_sample=False,\n",
        "    )\n",
        "\n",
        "input_len = inputs['input_ids'].shape[1]\n",
        "response = processor.tokenizer.decode(\n",
        "    output_ids[0][input_len:], skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(f\"\\nModel output:\\n{response}\")\n",
        "\n",
        "try:\n",
        "    parsed = json.loads(response)\n",
        "    exp = json.loads(expected)\n",
        "    print(f\"\\n{'Field':<20} {'Expected':>15} {'Got':>15} {'Match':>6}\")\n",
        "    print(\"-\" * 60)\n",
        "    for key in ['meter_number', 'full_reading', 'billing_reading', 'meter_type']:\n",
        "        e = exp.get(key, 'N/A')\n",
        "        g = parsed.get(key, 'N/A')\n",
        "        m = '✅' if str(e) == str(g) else '❌'\n",
        "        print(f\"{key:<20} {str(e):>15} {str(g):>15} {m:>6}\")\n",
        "    print(\"\\n✅ Valid JSON output!\")\n",
        "except json.JSONDecodeError:\n",
        "    print(\"\\n⚠️ Output is not valid JSON. May need more training.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dR-iQukpebDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCRl3UwdaZq1"
      },
      "source": [
        "---\n",
        "## Cell 12: Download Model\n",
        "\n",
        "**WHY:** Colab's disk is temporary. Download the LoRA weights (~80MB) before session ends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3odWRlZaZq1"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive(\"/content/parsight_meter_lora\", 'zip', SAVE_DIR)\n",
        "\n",
        "zip_file = \"/content/parsight_meter_lora.zip\"\n",
        "print(f\"Model ZIP: {os.path.getsize(zip_file)/1e6:.1f} MB\")\n",
        "print(\"Downloading...\")\n",
        "\n",
        "files.download(zip_file)\n",
        "print(\"\\n✅ Download started!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKh95RfeaZq1"
      },
      "source": [
        "---\n",
        "## Cell 13: Local Inference Script (save as meter_inference.py)\n",
        "\n",
        "**Usage on your Mac:**\n",
        "```bash\n",
        "pip install transformers peft torch qwen-vl-utils Pillow\n",
        "unzip parsight_meter_lora.zip -d ./parsight_meter_lora\n",
        "python meter_inference.py meter_photo.jpg\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8u1skD3naZq1"
      },
      "outputs": [],
      "source": [
        "INFERENCE_CODE = '''#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Parsight Meter Reading - Local Inference\n",
        "Usage: python meter_inference.py <image_path>\n",
        "\"\"\"\n",
        "import sys, json, torch\n",
        "from PIL import Image\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from peft import PeftModel\n",
        "\n",
        "BASE_MODEL = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "LORA_PATH = \"./parsight_meter_lora\"\n",
        "PROMPT = (\n",
        "    \"Extract the meter reading from this gas meter image. \"\n",
        "    \"Return a JSON object with meter_number, bp_number, full_reading, \"\n",
        "    \"billing_reading, meter_type, manufacturer, black_digits, red_digits, \"\n",
        "    \"and customer_name.\"\n",
        ")\n",
        "\n",
        "def load_model():\n",
        "    print(\"Loading model...\")\n",
        "    if torch.cuda.is_available():\n",
        "        device, dtype = \"cuda\", torch.float16\n",
        "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        device, dtype = \"mps\", torch.float16\n",
        "    else:\n",
        "        device, dtype = \"cpu\", torch.float32\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        BASE_MODEL, torch_dtype=dtype,\n",
        "        device_map=\"auto\" if device != \"cpu\" else None,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(model, LORA_PATH)\n",
        "    model.eval()\n",
        "    processor = AutoProcessor.from_pretrained(\n",
        "        LORA_PATH, min_pixels=256*28*28, max_pixels=512*28*28,\n",
        "    )\n",
        "    return model, processor\n",
        "\n",
        "def extract_reading(model, processor, image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    messages = [{\"role\": \"user\", \"content\": [\n",
        "        {\"type\": \"image\", \"image\": image},\n",
        "        {\"type\": \"text\", \"text\": PROMPT},\n",
        "    ]}]\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "    resp = processor.tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
        "    try: return json.loads(resp)\n",
        "    except: return {\"raw\": resp, \"error\": \"JSON parse failed\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if len(sys.argv) < 2: print(\"Usage: python meter_inference.py <image>\"); sys.exit(1)\n",
        "    model, proc = load_model()\n",
        "    print(json.dumps(extract_reading(model, proc, sys.argv[1]), indent=2))\n",
        "'''\n",
        "\n",
        "with open('/content/meter_inference.py', 'w') as f:\n",
        "    f.write(INFERENCE_CODE)\n",
        "\n",
        "files.download('/content/meter_inference.py')\n",
        "print(\"✅ Inference script ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "# Upload a test image\n",
        "print(\"Upload a meter image:\")\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "print(f\"\\nTesting: {image_path}\")\n",
        "\n",
        "# Load image\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Create prompt\n",
        "PROMPT = (\n",
        "    \"Extract the meter reading from this gas meter image. \"\n",
        "    \"Return a JSON object with meter_number, bp_number, full_reading, \"\n",
        "    \"billing_reading, meter_type, manufacturer, black_digits, red_digits, \"\n",
        "    \"and customer_name.\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": PROMPT},\n",
        "        ],\n",
        "    }\n",
        "]\n",
        "\n",
        "# Process\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "\n",
        "# Decode\n",
        "input_len = inputs['input_ids'].shape[1]\n",
        "response = processor.tokenizer.decode(output_ids[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nModel output:\\n{response}\")\n",
        "\n",
        "# Try parsing JSON\n",
        "try:\n",
        "    parsed = json.loads(response)\n",
        "    print(\"\\n✅ Valid JSON!\")\n",
        "    for k, v in parsed.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "except:\n",
        "    print(\"\\n⚠️ Raw text output (not valid JSON)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 744
        },
        "id": "a29njilRecjZ",
        "outputId": "893e7560-af8c-49c1-8a39-038a1bdddbe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a meter image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-52fc5c15-ed08-4c40-8686-05a6604f2cb3\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-52fc5c15-ed08-4c40-8686-05a6604f2cb3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2026-02-18 at 17.10.33.jpeg to WhatsApp Image 2026-02-18 at 17.10.33.jpeg\n",
            "\n",
            "Testing: WhatsApp Image 2026-02-18 at 17.10.33.jpeg\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:612: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model output:\n",
            "{\n",
            "  \"customer_name\": \"Indraprastha Gas Limited\",\n",
            "  \"utility_reading\": \"396.6\",\n",
            "  \"utility_reading_number\": \"17619818\",\n",
            "  \"utility_reading_type\": \"G1.6\",\n",
            "  \"utility_reading_blown\": \"396.6\",\n",
            "  \"utility_reading_reading\": \"396.6\",\n",
            "  \"utility_reading_blown_reading\": \"396.6\",\n",
            "  \"utility_reading_blown_type\": \"G1.6\",\n",
            "  \"utility_reading_blown_number\": \"17619818\",\n",
            "  \"utility_reading_reading_blown\": \"396.6\",\n",
            "  \"utility_reading_reading_blown_type\": \"G1.6\",\n",
            "  \"utility_reading_reading_blown_number\": \"17619818\"\n",
            "}\n",
            "\n",
            "✅ Valid JSON!\n",
            "  customer_name: Indraprastha Gas Limited\n",
            "  utility_reading: 396.6\n",
            "  utility_reading_number: 17619818\n",
            "  utility_reading_type: G1.6\n",
            "  utility_reading_blown: 396.6\n",
            "  utility_reading_reading: 396.6\n",
            "  utility_reading_blown_reading: 396.6\n",
            "  utility_reading_blown_type: G1.6\n",
            "  utility_reading_blown_number: 17619818\n",
            "  utility_reading_reading_blown: 396.6\n",
            "  utility_reading_reading_blown_type: G1.6\n",
            "  utility_reading_reading_blown_number: 17619818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb9C_1jKaZq1"
      },
      "source": [
        "---\n",
        "## Done! 🎉\n",
        "\n",
        "### You now have:\n",
        "1. `parsight_meter_lora.zip` — Fine-tuned LoRA weights (~80MB)\n",
        "2. `meter_inference.py` — Local inference script\n",
        "\n",
        "### Next steps:\n",
        "1. Test locally: `python meter_inference.py test_image.jpg`\n",
        "2. If accuracy ≥ 90%, replace GPT-4o endpoint in Parsight API\n",
        "3. If accuracy < 90%, collect more images and retrain\n",
        "\n",
        "### Cost savings:\n",
        "- GPT-4o: ~₹8 per reading\n",
        "- This model: ~₹0.08 per reading (100x cheaper)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume training for 3 more epochs with LOWER learning rate\n",
        "# WHY lower LR: Model already learned the basics. Now we fine-tune gently.\n",
        "# 1e-4 instead of 2e-4 = half the step size = less risk of overfitting.\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args_v2 = TrainingArguments(\n",
        "    output_dir=\"/content/parsight_meter_model_v2\",\n",
        "\n",
        "    num_train_epochs=3,\n",
        "\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "\n",
        "    learning_rate=1e-4,            # HALF of before (was 2e-4)\n",
        "    warmup_ratio=0.05,             # Less warmup (model is already warmed up)\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    weight_decay=0.01,\n",
        "\n",
        "    fp16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    optim=\"adamw_torch\",\n",
        "    max_grad_norm=1.0,\n",
        "\n",
        "    logging_steps=5,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "\n",
        "    remove_unused_columns=False,\n",
        "    dataloader_pin_memory=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer_v2 = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_v2,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "print(\"Resuming training (3 more epochs, LR=1e-4)...\")\n",
        "print(\"Watch eval_loss — if it INCREASES, we're overfitting.\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = trainer_v2.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Train loss: {result.training_loss:.4f}\")\n",
        "eval_r = trainer_v2.evaluate()\n",
        "print(f\"Val loss:   {eval_r['eval_loss']:.4f}\")\n",
        "\n",
        "# Save updated model\n",
        "model.save_pretrained(\"/content/parsight_meter_lora_v2\")\n",
        "processor.save_pretrained(\"/content/parsight_meter_lora_v2\")\n",
        "print(\"\\n✅ Saved to /content/parsight_meter_lora_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "_PfrE2_viSqH",
        "outputId": "53f420e6-7334-4ab2-caae-7d6e931261b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training (3 more epochs, LR=1e-4)...\n",
            "Watch eval_loss — if it INCREASES, we're overfitting.\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='24' max='24' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [24/24 02:23, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.572600</td>\n",
              "      <td>0.431517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.359200</td>\n",
              "      <td>0.352497</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.260500</td>\n",
              "      <td>0.326005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Train loss: 0.3824\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8/8 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val loss:   0.3260\n",
            "\n",
            "✅ Saved to /content/parsight_meter_lora_v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Zip v2 model\n",
        "shutil.make_archive(\"/content/parsight_meter_lora_v2\", 'zip', \"/content/parsight_meter_lora_v2\")\n",
        "files.download(\"/content/parsight_meter_lora_v2.zip\")\n",
        "\n",
        "# Also download inference script\n",
        "files.download(\"/content/meter_inference.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "Nx-WEPsniVMw",
        "outputId": "c1240434-7031-4631-abc2-47b95e046535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5fbe7eaf-bb21-484b-92c5-e96a30e79d9a\", \"parsight_meter_lora_v2.zip\", 20010305)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: /content/meter_inference.py",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-253616752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Also download inference script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/meter_inference.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    228\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: /content/meter_inference.py"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Upload a meter image:\")\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]\n",
        "print(f\"\\nTesting: {image_path}\")\n",
        "\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "PROMPT = (\n",
        "    \"Extract the meter reading from this gas meter image. \"\n",
        "    \"Return a JSON object with meter_number, bp_number, full_reading, \"\n",
        "    \"billing_reading, meter_type, manufacturer, black_digits, red_digits, \"\n",
        "    \"and customer_name.\"\n",
        ")\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": [\n",
        "    {\"type\": \"image\", \"image\": image},\n",
        "    {\"type\": \"text\", \"text\": PROMPT},\n",
        "]}]\n",
        "\n",
        "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor(text=[text], images=[image], return_tensors=\"pt\", padding=True).to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=256, do_sample=False)\n",
        "\n",
        "input_len = inputs['input_ids'].shape[1]\n",
        "response = processor.tokenizer.decode(output_ids[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "print(f\"\\nModel output:\\n{response}\")\n",
        "\n",
        "try:\n",
        "    # Handle markdown code blocks\n",
        "    if \"```\" in response:\n",
        "        json_str = response.split(\"```\")[1].replace(\"json\", \"\").strip()\n",
        "        parsed = json.loads(json_str)\n",
        "    else:\n",
        "        parsed = json.loads(response)\n",
        "    print(\"\\n✅ Valid JSON!\")\n",
        "    for k, v in parsed.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "except:\n",
        "    print(\"\\n⚠️ Raw text output\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "txykDU2QjFp4",
        "outputId": "607e7550-385d-4b17-a5ab-0b198d0d6a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload a meter image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4711dc75-2f11-4e72-833f-72cb01ccc1bd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4711dc75-2f11-4e72-833f-72cb01ccc1bd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving WhatsApp Image 2026-02-18 at 17.10.33.jpeg to WhatsApp Image 2026-02-18 at 17.10.33 (1).jpeg\n",
            "\n",
            "Testing: WhatsApp Image 2026-02-18 at 17.10.33 (1).jpeg\n",
            "\n",
            "Model output:\n",
            "{\n",
            "  \"customer_name\": \"Indraprastha Gas Limited\",\n",
            "  \"utility_reading\": \"396.6\",\n",
            "  \"utility_reading_number\": \"17619818\",\n",
            "  \"utility_reading_type\": \"G1.6\",\n",
            "  \"utility_reading_blown\": \"396.6\",\n",
            "  \"utility_reading_reading\": \"396.6\",\n",
            "  \"utility_reading_blown_reading\": \"396.6\",\n",
            "  \"utility_reading_blown_type\": \"G1.6\",\n",
            "  \"utility_reading_blown_number\": \"17619818\",\n",
            "  \"utility_reading_reading_blown\": \"396.6\",\n",
            "  \"utility_reading_reading_blown_type\": \"G1.6\",\n",
            "  \"utility_reading_reading_blown_number\": \"17619818\"\n",
            "}\n",
            "\n",
            "✅ Valid JSON!\n",
            "  customer_name: Indraprastha Gas Limited\n",
            "  utility_reading: 396.6\n",
            "  utility_reading_number: 17619818\n",
            "  utility_reading_type: G1.6\n",
            "  utility_reading_blown: 396.6\n",
            "  utility_reading_reading: 396.6\n",
            "  utility_reading_blown_reading: 396.6\n",
            "  utility_reading_blown_type: G1.6\n",
            "  utility_reading_blown_number: 17619818\n",
            "  utility_reading_reading_blown: 396.6\n",
            "  utility_reading_reading_blown_type: G1.6\n",
            "  utility_reading_reading_blown_number: 17619818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-x0XjNhkbRd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}